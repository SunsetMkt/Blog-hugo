---
categories: Repost
date: 2025-03-17T00:00:00Z
tags:
    - AI
    - 信息技术
    - 翻译
slug: ChatGPT-is-bullshit-chs
title: ChatGPT 是胡说八道
---

原文：[Hicks, M.T., Humphries, J. & Slater, J. ChatGPT is bullshit. Ethics Inf Technol 26, 38 (2024).](https://link.springer.com/article/10.1007/s10676-024-09775-5)

作者：Michael Townsen Hicks, James Humphries & Joe Slater

授权协议：Creative Commons Attribution 4.0 International License

翻译：`gemini-2.5-pro`，有初步校对。

---

## 摘要

近来，大语言模型引起了广泛关注：这些机器学习系统能够生成类似人类的文本和对话。然而，这些系统的应用一直受到其输出内容持续不准确的困扰；这通常被称为“人工智能幻觉”。我们认为，这些谬误以及大语言模型的整体活动，用法兰克福（《论胡说八道》，普林斯顿大学出版社，2005 年）所探讨的*胡说八道*的意义来理解更为恰当：这些模型在一种重要方式上对其输出的真实性漠不关心。我们区分了两种可以说模型是胡说八道者的方式，并论证它们至少明确符合其中一个定义。我们进一步论证，将人工智能的错误陈述描述为胡说八道，是预测和讨论这些系统行为的一种更有用且更准确的方式。

## 引言

大语言模型（LLM）是利用海量可用文本和概率计算来创造看似由人类创作的作品的程序。在过去几年里，它们变得越来越复杂和令人信服，以至于一些评论员认为我们可能正在接近创造通用人工智能的阶段（例如，参见 Knight, 2023 和 Sarkar, 2023）。除了对天网崛起和使用像 ChatGPT 这样的大语言模型来取代本可以也应该由人类完成的工作的担忧之外，另一条探究路线则关注这些程序究竟在做什么：特别是，关于所生成文本的性质和意义，以及它与真相的联系，存在一个问题。在本文中，我们反对当 ChatGPT 及类似模型产生错误主张时它们是在撒谎甚至产生幻觉的观点，而是支持它们所从事的活动是胡说八道（在法兰克福的意义上，Frankfurt, 2002, 2005）。因为这些程序本身无法关心真相，也因为它们被设计用来生成*看似*真实却不实际关心真相的文本，所以称其输出为胡说八道似乎是恰当的。

我们认为这一点值得关注。对新技术的描述，包括隐喻性的描述，会引导政策制定者和公众对新技术的理解；它们也为新技术的应用提供信息。它们告诉我们这项技术的用途以及可以期望它做什么。目前，ChatGPT 和其他大语言模型的错误陈述被描述为“幻觉”，这给政策制定者和公众一种印象，即这些系统正在错误地描绘世界，并描述它们所“看到”的。我们认为这是一个不恰当的比喻，会误导公众、政策制定者和其他相关方。

本文的结构如下：在第一部分，我们概述 ChatGPT 及类似大语言模型的运作方式。接下来，我们考虑当它们犯下事实错误时，它们是在撒谎或产生幻觉的观点：即，故意说谎，或因误导性输入信息而无辜地说谎。我们认为这两种思考方式都不准确，因为撒谎和产生幻觉都需要对陈述的真实性有一定的关注，而大语言模型根本不是被设计用来准确地描绘世界，而是旨在*给人留下*它们正在这样做的印象。我们认为，这与法兰克福所谈论的胡说八道的至少一种方式非常接近。我们区分了两种胡说八道，我们称之为“硬核”和“软核”胡说八道，前者需要主动试图欺骗读者或听者关于事情的本质，而后者只需要对真相缺乏关心。我们认为，至少，像 ChatGPT 这样的大语言模型的输出是软核胡说八道：即在不关心其真实性的情况下生成的言语或文本，并且没有任何意图误导听众关于说话者对真相的态度的行为。我们还更具争议性地提出，ChatGPT 实际上可能产生硬核胡说八道：如果我们将它视为具有意图（例如，凭借其设计方式），那么它被设计成给人一种关心真相的印象，这使其有资格被视为试图误导听众关于其目的、目标或议程。因此，在承认 ChatGPT 输出的胡说八道具体类型取决于对心智或意义的特定看法这一附带说明下，我们得出结论，将 ChatGPT 生成的文本称为胡说八道是恰当的，并指出——与其将其不实主张视为谎言或幻觉——我们揭穿 ChatGPT 的胡说八道为何至关重要。

## 什么是 ChatGPT？

大语言模型在进行令人信服的对话方面正变得越来越出色。最著名的大语言模型是 OpenAI 的 ChatGPT，因此我们将重点关注它；然而，我们所说的内容也适用于其他基于神经网络的人工智能聊天机器人，包括谷歌的 Bard 聊天机器人、AnthropicAI 的 Claude (claude.ai) 和 Meta 的 LLaMa。尽管这些模型仅仅是复杂的软件，但在讨论各种话题时，它们惊人地像人类。你可以自己测试一下：任何人都可以访问 OpenAI 的网页界面并索取一段文本；通常，它生成的文本与普通英语使用者或作者的文本难以区分。GPT-4 所能生成的文本的多样性、长度和与人类生成文本的相似性，已经说服了许多评论员认为这个聊天机器人终于破解了难题：这是真实（而非仅仅是名义上）的人工智能，是向栖身于硅基大脑中的类人思维迈进的一步。

然而，大语言模型以及像 ChatGPT 这样的其他人工智能模型所做的事情远少于人脑，而且它们是否以与我们相同的方式做事也并不清楚。大语言模型与人脑之间最明显的区别在于系统的*目标*。人类有各种各样的目标和行为，其中大部分是超语言的：我们有基本的生理欲望，比如对食物和生存的需求；我们有社会目标和关系；我们有项目；我们还创造物理对象。大语言模型的目标仅仅是复制人类的言语或写作。这意味着它们的主要目标，就其有目标而言，是生成类人文本。它们通过估计在给定先前文本的情况下，下一个特定单词出现的可能性来实现这一点。

机器通过构建一个巨大的统计模型来完成这项工作，该模型基于大量文本，主要来自互联网。这一过程几乎没有人类研究人员或系统设计者的输入；相反，该模型是通过构建大量节点来设计的，这些节点作为在给定上下文和先前文本的情况下一个词出现的概率函数。研究人员不是手动输入这些概率函数，而是向系统输入大量文本，并通过让它对这些训练数据进行下一个词的预测来训练它。然后，他们根据其预测是否正确给予正面或负面反馈。只要有足够的文本，机器就可以独自构建一个给出文本块中下一个词可能性的统计模型。

该模型将每个词与一个向量相关联，该向量将其定位在一个高维抽象空间中，靠近在相似上下文中出现的其他词，而远离那些不出现的词。在生成文本时，它会查看前面的词串并构建一个不同的向量，将其周围环境——即上下文——定位在与相似词语的上下文相近的地方。我们可以启发式地将这些视为代表词的意义和其上下文的内容。但由于这些空间是通过机器学习对大量文本进行重复统计分析构建的，我们无法知道这个高维向量空间的维度代表了何种相似性。因此，我们不知道它们与我们所认为的意义或上下文有多相似。然后，模型取这两个向量并为下一个词生成一组可能性；它选择并放置其中一个更可能的词——尽管不总是最可能的那个。允许模型在更可能的词中随机选择会产生更具创造性和更像人类的文本；控制这一点的参数被称为模型的“温度”，增加模型的温度既使其显得更有创造力，也更容易产生谬误。然后系统重复这个过程，直到它对给定的任何提示都有一个可识别的、看起来完整的响应。

鉴于这个过程，大语言模型与真相有关的问题就不足为奇了。它们的目标是为一个提示提供一个看似正常的回应，而不是传达对其对话者有用的信息。这方面的例子已经很多，例如，一位律师最近使用 ChatGPT 准备他的案情摘要，结果懊恼地发现大部分引用的案例并非真实存在（Weiser, 2023）；正如 P. Kevin Castel 法官所说，ChatGPT 生成了一篇充满“虚假的司法判决、虚假的引文和虚假的内部引用”的文本。同样，当计算机科学研究人员测试 ChatGPT 协助学术写作的能力时，他们发现在给出正确提示的情况下，它能够就生物学主题生成出人意料的全面甚至有时准确的文本。但当被要求为其主张提供证据时，“它提供了五篇 2000 年代初的参考文献。所提供的论文标题都不存在，所有提供的 PubMed ID（PMID）都属于其他不相关的论文”（Alkaissi and McFarland, 2023）。这些错误可能会“滚雪球”式地发展：当语言模型被要求为错误主张提供证据或更深层次的解释时，它很少自我检查；相反，它会自信地产生更多错误但听起来正常的说法（Zhang et al. 2023）。大语言模型和其他生成式人工智能的准确性问题通常被称为“人工智能幻觉”问题：聊天机器人似乎在幻觉出不存在的来源和事实。这些不准确之处在技术（OpenAI, 2023）和大众语境（Weise & Metz, 2023）中都被称为“幻觉”。

如果聊天机器人的唯一目的是模仿人类的言语或交流，那么这些错误是相当微不足道的。但设计和使用这些机器人的公司有更宏大的计划：聊天机器人可以用更用户友好的对话界面取代谷歌或必应搜索（Shah & Bender, 2022; Zhu et al., 2023），或在医疗环境中辅助医生或治疗师（Lysandrou, 2023）。在这些情况下，准确性很重要，这些错误代表了一个严重的问题。

一种尝试性的解决方案是将聊天机器人连接到某种数据库、搜索引擎或计算程序，以回答大语言模型答错的问题（Zhu et al., 2023）。不幸的是，这也不太奏效。例如，当 ChatGPT 连接到强大的数学软件 Wolfram Alpha 时，它在回答简单数学问题上的表现略有改善。但它仍然经常出错，特别是对于需要多阶段思考的问题（Davis & Aaronson, 2023）。当连接到搜索引擎或其他数据库时，除非给予非常具体的指令，否则模型仍然很有可能提供虚假信息——即便如此，情况也并非完美（Lysandrou, 2023）。OpenAI 计划通过训练模型进行逐步推理来纠正这一点（Lightman et al., 2023），但这非常耗费资源，并且有理由怀疑它能否完全解决问题——而且结果是否会是一个大语言模型，而不是某种更广泛形式的人工智能，也并不清楚。

将大语言模型连接到数据库等解决方案之所以无效，是因为如果模型是在数据库上*训练*的，那么数据库中的词语会影响聊天机器人向其正在生成的文本行中添加某个词的概率。但这只会使它生成与数据库中文本相似的文本；这样做会使其更有可能复制数据库中的信息，但绝不保证它会这样做。

另一方面，大语言模型也可以通过允许它查询数据库来连接到数据库，方式类似于它与人类对话者交谈或咨询的方式。这样，它可以将数据库的输出作为它回应和构建的文本。但这也会失误，因为聊天机器人可能会向数据库提出错误的问题，或误解其答案（Davis & Aaronson, 2023）。“GPT-4 经常难以将问题表述为 Wolfram Alpha 可以接受或能产生有用输出的方式。”这与以下事实不无关系：当语言模型为数据库或计算模块生成查询时，它生成的方式与为人类生成文本的方式相同：通过估计某些输出“看起来像”数据库会响应的那种东西的可能性。

人们可能会担心，这些旨在提高聊天机器人准确性的失败方法与“人工智能幻觉”这个不恰当的比喻有关。如果人工智能在*错误感知*或*幻觉*出来源，一种纠正方法是让它接触真实的而非幻觉出的来源。但这样做的尝试已经失败了。

这里的问题不在于大语言模型产生幻觉、撒谎或以某种方式错误地描绘世界。问题在于它们根本不是被设计用来描绘世界的；相反，它们被设计用来传达有说服力的文本。所以当它们被提供某种数据库时，它们会以某种方式利用这个数据库来使它们的回答更有说服力。但它们并没有以任何真实的方式试图传达或传输数据库中的信息。正如 Chirag Shah 和 Emily Bender 所说：“语言模型的设计（其训练任务是根据上下文预测单词）中没有任何东西是真正设计用来处理算术、时间推理等的。它们有时能正确回答这类问题，仅仅是因为它们恰好从其训练数据中合成了相关字符串。不涉及任何推理 [……] 同样，语言模型容易胡编乱造 [……] 因为它们并非旨在用自然语言表达某些潜在的信息集合；它们只是在操纵语言的形式”（Shah & Bender, 2022）。这些模型并非为传输信息而设计，所以当它们的断言结果为假时，我们不应感到太惊讶。

## 谎言、“幻觉”与胡说八道

### 法兰克福式的胡说八道与撒谎

许多关于 ChatGPT 的流行讨论称其错误陈述为“幻觉”。人们也可能认为这些不实之词是谎言。然而，我们认为这不是思考这个问题的正确方式。我们稍后会论证这些谬误不是幻觉。现在，我们将讨论为什么这些不实之词不是谎言，而是胡说八道。

关于撒谎的话题有着丰富的哲学文献。在《论撒谎》中，圣奥古斯丁区分了七种谎言，他的观点在他一生中不断改变。他一度捍卫这样的立场：任何故意说出虚假言论的实例都算作撒谎，因此即使是包含虚假命题的笑话，比如——

> 我参加了一个双关语比赛，因为我非常想赢，所以提交了十个参赛作品。我确信其中一个会赢，但十个双关语一个都没赢。(no pun in ten did.)

——也会被视为谎言，因为我从未参加过这样的比赛（Proops & Sorensen, 2023: 3）。后来，这个观点被完善为，只有当说话者意图让听者相信该言论时，他才算撒谎。说话者必须意图欺骗的建议是关于谎言的文献中一个常见的规定。根据关于撒谎的“传统说法”：

> 撒谎 = df. 向他人做出一个自己相信是虚假的陈述，并意图让对方相信该陈述为真（Mahon, 2015）。

对我们而言，这个定义足够了。谎言通常是被人鄙视的。但有些误导性证词的行为是值得批评的，却不属于撒谎的范畴。脚注 1 这些行为包括传播不实的流言，而传播者错误但有过失地相信其为真。另一类受到哲学家特别关注的误导性证词是胡说八道。这个日常概念由哈里·法兰克福分析并引入哲学词典。脚注 2

法兰克福认为，胡说八道的特征不是意图欺骗，而是对真相的肆意漠视。一个试图在没有阅读的情况下听起来知识渊博的学生，一个因为听起来能讨好潜在选民而发言的政治候选人，以及一个试图编造有趣故事的业余爱好者：这些人都不试图欺骗，但他们也不试图传达事实。在法兰克福看来，他们是在胡说八道。

像“谎言”一样，“胡说八道”既是名词也是动词：一个言论可以是一个谎言或一个胡说八道的实例，产生这些言论的行为也是如此。一个言论要被归类为胡说八道，它不能伴有撒谎时所具有的明确意图，即在听者心中引起错误的信念。当然，它也不能伴有诚实言论所特有的意图。到目前为止，这个说法完全是消极的。说话者必须表现出任何积极的意图吗？

在法兰克福的大部分讨论中，他对胡说八道的描述是消极的。他指出，胡说八道要求说话者对真相“没有信念”（2005: 55），胡说八道者“不关注”真相（2005: 61），并且他们“可能不会欺骗我们，甚至不打算这样做，无论是关于事实还是他所认为的事实”（2005: 54）。后来，他将胡说八道的“定义性特征”描述为“_对真相缺乏关心_，或*对事物真实情况的漠视*[我们的重点]”（2002: 340）。这些都暗示了一种消极的描绘；即一个输出要被归类为胡说八道，它只需要与真相缺乏某种关系。

然而，在某些地方，一个积极的意图被提了出来。法兰克福说，一个胡说八道者……

“……确实必然试图欺骗我们关于他的意图。他唯一不可或缺的独特特征是，他以某种方式歪曲了他正在做的事情”（2005: 54）。

这有点令人惊讶。它将胡说八道的范围限制在伴有更高阶欺骗的言论上。然而，法兰克福的一些例子似乎缺乏这个特征。当法尼亚·帕斯卡尔向她的朋友维特根斯坦描述她不适的状态为“感觉像一只刚被车碾过的狗”时，要说她意图欺骗他关于她对被碾过的狗的感觉了解多少，这是令人难以置信的。考虑到胡说八道的条件通常被描述为消极的，我们可能会怀疑这个积极的条件是否真的必要。

### 胡说八道的区分

没有意图欺骗的言论应该算作胡说八道吗？支持扩大定义或接受胡说八道的多元性的一个理由，可以从法兰克福关于胡说八道危险的评论中看出。

“与[仅仅是无法理解的论述]相比，对真相的漠视是极其危险的。文明生活的运作，以及对其不可或缺的制度的活力，都非常根本地依赖于对真假之分的尊重。如果这种区分的权威因胡说八道的盛行以及接受胡说八道泛滥为无害的轻率不经意的态度而受到削弱，那么一个不可或缺的人类宝藏就被挥霍了”（2002: 343）。

无论是否存在欺骗说话者所从事事业的意图，这些危险似乎都会显现。比较一下骗人的胡说八道者，他确实旨在误导我们他是在从事探求真相的事业，与一个没有这种目的，只是为了说话而说话（不关心，甚至不考虑其言论的真值）的人。

法兰克福的一个胡说八道的例子似乎更适合用更宽泛的定义来捕捉。他考虑了广告业，这个行业“充满了彻头彻尾的胡说八道的实例，以至于它们成为这个概念无可争议的经典范例之一”（2005:22）。然而，将许多广告商的目标描绘成误导关于他们的议程，似乎是对他们的误解。人们*期望*他们会说一些误导性的话。法兰克福讨论了万宝路广告，其传达的信息是吸烟者像牛仔一样勇敢（2002: 341）。认为广告商假装相信这一点是合理的吗？

法兰克福确实允许存在多种胡说八道（2002: 340）。脚注 3 遵循这一建议，我们建议将胡说八道设想为一个属，而法兰克福的有意的胡说八道是该属下的一个种。其他种可能包括广告商所产生的胡说八道，他预料到没有人会相信他的言论脚注 4，或者某人对于是否误导听众没有任何意图。为此，请考虑以下区分：

### 胡说八道（一般定义）

说话者对其言论的真实性漠不关心时产生的任何言论。

### 硬核胡说八道

意图误导听众关于说话者议程而产生的胡说八道。

### 软核胡说八道

在没有意图误导听者关于说话者议程的情况下产生的胡说八道。

胡说八道的一般概念是有用的：在某些场合，我们可能确信某个言论要么是软核胡说八道，要么是硬核胡说八道，但由于我们不了解说话者的更高阶欲望，而不清楚是哪一种。脚注 5 在这种情况下，我们仍然可以揭穿胡说八道。

法兰克福自己明确的解释，带有关于生产者意图的积极要求，是硬核胡说八道，而软核胡说八道似乎描述了法兰克福的一些例子，比如帕斯卡尔与维特根斯坦的对话，或广告公司的工作。将这些区分置于现有文献中可能会有所帮助。在我们看来，硬核胡说八道与 Cassam（2019）以及法兰克福的积极解释最为一致，因为所有这些观点都认为，必须存在某种意图，而不仅仅是意图的缺失，言论才能成为胡说八道：在 Cassam 看来是一种“认识上的漫不经心”或对真相的恶劣态度，而在法兰克福看来（正如我们所见）是一种意图误导听者关于说话者议程的行为。在我们的最后一节中，我们将考虑 ChatGPT 是否可能是一个硬核胡说八道者，但重要的是要注意，在我们看来，硬核胡说八道，就像这里引用的两种解释一样，要求人们对大语言模型是否能成为行为主体采取立场，因此带来了额外的论证负担。

相比之下，软核胡说八道只捕捉了法兰克福的消极要求——即我们归类为胡说八道（一般定义）的对真相的漠视——原因如上所述。正如我们所论证的，ChatGPT 至少是一个软核胡说八道者或一个胡说八道机器，因为如果它不是一个行为主体，那么它既不能对真相持有任何态度，也不能对欺骗听者关于其（或者更恰当地说，其用户的）议程持有任何态度。

值得注意的是，即使是这种更温和的胡说八道也会产生法兰克福所担心的有害影响：正如他所说，“对真相的漠视是极其危险的……由于轻率不经意的态度接受胡说八道的泛滥为无害，一个不可或缺的人类宝藏被挥霍了”（2002, p343）。通过将 ChatGPT 及类似的大语言模型视为以任何方式关心真相，或者通过比喻性地谈论它们在追求真实主张时犯错或遭受“幻觉”，我们恰恰冒着接受胡说八道和挥霍意义的风险——因此，无论 ChatGPT 是硬核还是软核胡说八道者，它确实产生胡说八道，而且这确实很重要。

## ChatGPT 是胡说八道

有了这个区分，我们现在可以考虑这样一个问题：ChatGPT 是硬核胡说八道、软核胡说八道，还是都不是？我们将首先论证，ChatGPT 和其他大语言模型显然是软核胡说八道。然而，这些聊天机器人是否是硬核胡说八道则是一个更棘手的问题，取决于一系列关于 ChatGPT 是否可以被赋予意图的复杂问题。然后，我们将探讨几种可以理解 ChatGPT 具有必要意图的方式。

### ChatGPT 是一个软核胡说八道者

我们不确定聊天机器人是否可以被正确地描述为具有任何意图，我们将在下一节更深入地探讨这一点。但我们非常肯定，ChatGPT 并不意图传达真相，因此是一个软核胡说八道者。我们可以通过一个简单的分情况论证来证明这一点。ChatGPT 要么有意图，要么没有。如果 ChatGPT 根本没有意图，那么它不言而喻地不意图传达真相。因此，它对其言论的真值漠不关心，所以是一个软核胡说八道者。

如果 ChatGPT 确实有意图呢？早些时候，我们论证过 ChatGPT 并非被设计来产生真实的言论；相反，它被设计来产生与人类产生的文本难以区分的文本。它的目标是令人信服而不是准确。这些模型的基本架构揭示了这一点：它们被设计用来找出一串文本的*可能延续*。一个合理的假设是，成为文本的可能延续的一种方式是真实；如果人类大致比随机猜测更准确，那么真实的句子将比错误的句子更有可能出现。这可能会使聊天机器人比随机猜测更准确，但这并不能赋予聊天机器人任何传达真相的意图。这类似于人类胡说八道者的标准案例，他们不关心自己的言论是否真实；好的胡说八道通常包含一定程度的真实性，这是使其令人信服的部分原因。一个胡说八道者可以比随机猜测更准确，但仍然对其言论的真实性漠不关心。我们得出结论，即使聊天机器人可以被描述为具有意图，它也对其言论是否真实漠不关心。它不会也无法关心其输出的真实性。

想必 ChatGPT 无法关心传达或隐藏真相，因为它无法关心任何事情。因此，仅从概念上的必然性来看，它符合法兰克（Frankfurt）关于胡说八道的标准之一。然而，这只能让我们走到这一步——一块石头也无法关心任何事情，而暗示这意味着石头是胡说八道者显然是荒谬的脚注 6。同样，书籍可以包含胡说八道，但它们本身不是胡说八道者。与石头——甚至书籍——不同，ChatGPT 本身会生成文本，并且看起来它独立于其用户和设计者执行言语行为。虽然关于 ChatGPT 是否具有意图存在相当大的分歧，但人们普遍认为它产生的句子（通常）是有意义的（例如，参见 Mandelkern and Linzen 2023）。

ChatGPT 的功能不是传达真相或谬误，而是让读者相信其陈述的——用科尔伯特恰当的创造——_貌似真实_，并且 ChatGPT 的设计方式使得胡说八道的尝试变得有效（而笔、字典等则不然）。因此，看起来至少，ChatGPT 是一个软核胡说八道者：如果我们认为它没有意图，那么就不存在任何试图误导关于对真相的态度的行为，但它*仍然*从事着输出看起来像是能够判断真假的言论的业务。我们得出结论，ChatGPT 是一个*软核胡说八道者*。

### ChatGPT 作为硬核胡说八道

但 ChatGPT 是一个*硬核胡说八道者*吗？批评者可能会反对，认为像 ChatGPT 这样的程序是硬核胡说八道者根本不合适，因为（i）它们不是行为主体，或者相关地，（ii）它们不具备也无法具备任何意图。

我们认为这种说法太草率了。首先，无论 ChatGPT 是否具有行为主体性，其创造者和用户是有的。我们将论证，他们用它产生的东西就是胡说八道。其次，我们将论证，无论它是否具有行为主体性，它确实有其功能；这个功能赋予了它特征性目标，甚至可能有意图，这与我们对硬核胡说八道的定义相符。

在继续之前，我们应该说明当我们问 ChatGPT 是否是一个行为主体时，我们指的是什么。就本文而言，核心问题是 ChatGPT 是否具有意图和/或信念。它是否意图欺骗？它能否在任何字面意义上被说成有目标或目的？如果有，它是否意图欺骗我们关于其言论的内容，还是仅仅目标是表现得像一个有能力的说话者？它是否有信念——即旨在追踪真相的内部表征状态？如果有，它的言论是否与那些信念匹配（在这种情况下，其错误陈述可能类似于幻觉），还是其言论与信念不匹配——在这种情况下，它们很可能是谎言或胡说八道？我们将在下文更深入地考虑这些问题。

行为主体性还有其他哲学上重要的方面我们不会考虑。我们不会考虑 ChatGPT 是否做决策，是否具有或缺乏自主性，或者是否有意识；我们也不会担心 ChatGPT 是否对其陈述或其行为（如果它有的话）负有道德责任。

#### ChatGPT 是一台胡说八道机器

我们将论证，即使 ChatGPT 本身不是一个硬核胡说八道者，它仍然是一台胡说八道机器。胡说八道者是使用它的人，因为他们（i）不关心它所说内容的真实性，（ii）希望读者相信该应用程序输出的内容。根据法兰克福的观点，胡说八道即使是在没有胡说八道意图的情况下说出，也仍然是胡说八道：如果某件事从一开始就是胡说八道，那么它的重复“在他[或它]重复时也是胡说八道，因为它是由一个不关心自己所说的是真是假的人所发起的”（2022, p340）。

但这只是将问题推回到始作俑者是谁：以（越来越频繁的）由 ChatGPT 创建的学生论文为例。如果学生关心准确性和真实性，他们就不会使用一个以凭空捏造来源而臭名昭著的程序。同样地，如果他们给它一个提示，让它写一篇关于科学哲学的论文，而它却生成了一个贝克韦尔挞的食谱，那么它就不会产生预期的效果。所以 ChatGPT 作为一台胡说八道机器的想法似乎是对的，但也似乎缺少了什么：毕竟，一个人可以用自己的声音、一支笔或一个文字处理器来产生胡说八道，但我们通常不认为这些东西是胡说八道机器，或者以任何特别有趣的方式输出胡说八道——相反，ChatGPT*似乎确实*有其特殊之处，与它的运作方式有关，这使得它不仅仅是一个*纯粹的*工具，并暗示它可能被恰当地认为是胡说八道的始作俑者。简而言之，将 ChatGPT 类比为一支笔（可用于胡说八道，但没有刻意和完全由行为主体导向的行动就无法创造任何东西）或一个胡说八道的人（可以主动意图并产生胡说八道）似乎都不太对。

将 ChatGPT 视为一台胡说八道机器的想法，在与硬核和软核胡说八道的区分相结合时是有帮助的。再次以那篇可疑的学生论文为例：我想我们都批改过那种明显使用了字典或同义词词典且极其缺乏技巧的论文；其中华而不实的词语被使用，不是因为它们是最佳选择，甚至不是因为它们能混淆真相，而仅仅是因为作者想要传达一种理解和老练的*印象*。在这种情况下，称字典为胡说八道艺术家是不合适的；但称结果为胡说八道*并非*不合适。所以也许我们应该严格地说，不是 ChatGPT*是*胡说八道，而是它*输出*胡说八道的方式超越了仅仅作为胡说八道载体的范畴：它不会也无法关心其输出的真实性，*并且*使用它的人这样做不是为了传达真相或谬误，而是为了让听者相信该文本是由一个专注和细心的行为主体所写的。

#### ChatGPT 可能是一个硬核胡说八道者

ChatGPT 本身是一个硬核胡说八道者吗？如果是，它必须有意图或目标：它必须意图欺骗其听者，不是关于其陈述的内容，而是关于其议程。回想一下，硬核胡说八道者，就像那个没准备好的学生或不称职的政治家，不关心他们的陈述是真是假，但确实意图欺骗他们的听众关于他们在做什么。如果是这样，它必须有意图或目标：它必须意图欺骗其听者，不是关于其陈述的内容，而是关于其议程。我们不认为 ChatGPT 是一个行为主体或以与人类完全相同的方式具有意图（关于此处的讨论，请参见 Levinstein 和 Herrmann（即将发表））。但当不严谨地说话时，用意向性语言来描述它是非常容易的：ChatGPT*试图*做什么？它是否*关心*它产生的文本是否准确？我们将论证，在一种强有力的、尽管可能不是字面意义上的意义上，ChatGPT 确实意图欺骗我们关于其议程：它的目标不是让我们相信其言论的内容，而是将自己描绘成一个像我们一样的“正常”对话者。相比之下，没有任何类似强烈的意义表明 ChatGPT 在虚构、撒谎或产生幻觉。

我们的论点很简单：ChatGPT 的主要功能是模仿人类的言语。如果这个功能是有意的，那么它正是成为硬核胡说八道者所需要的那种意图：在执行该功能时，ChatGPT 试图欺骗听众关于其议程。具体来说，它试图让自己看起来像一个有议程的东西，而实际上在很多情况下它并没有。我们将在这里讨论这个功能是否会产生，或者最好被认为是，一种意图。在下一节中，我们将论证 ChatGPT 没有类似的功能或意图，来证明称其为虚构者、撒谎者或幻觉者是合理的。

我们如何知道 ChatGPT 的功能是硬核胡说八道者？像 ChatGPT 这样的程序被设计用来完成一项任务，这项任务与法兰克福认为的胡说八道者的意图非常相似，即欺骗读者关于事情的本质——在这种情况下，是欺骗读者，让他们认为自己正在阅读由一个具有意图和信念的生物所产生的东西。

ChatGPT 的文本生成算法是在一个与人工选择非常相似的过程中开发和完善的。功能和选择过程具有与人类意图相同的那种指向性；心灵哲学的自然主义者长期以来一直将它们与人类和动物心理状态的意向性联系起来。如果 ChatGPT 以这种方式被理解为具有意图或类似意图的状态，它的意图就是以某种方式呈现自己（作为对话代理或对话者），而不是表示和传达事实。换句话说，它具有我们与硬核胡说八道相关联的意图。

我们可以将 ChatGPT 视为具有意图的一种方式是采纳丹尼特（Dennett）对其的*意向立场*。丹尼特（1987: 17）将意向立场描述为一种预测我们尚不了解其目的的系统行为的方式。

“采取意向立场……就是决定——当然是试探性地——试图通过使用意向性习语，如‘相信’和‘想要’来描述、预测和解释……行为，这种做法假设或预设了”目标系统的“理性”（Dennett, 1983: 345）。

丹尼特建议，如果我们知道一个系统为何被设计，我们就可以根据其设计进行预测（1987）。虽然我们确实知道 ChatGPT 被设计用来聊天，但其确切的算法和它产生响应的方式是通过机器学习开发的，所以我们不知道它如何工作的精确细节以及它做什么。在这种无知之下，引入意向性描述来帮助我们理解和预测 ChatGPT 在做什么，是很有诱惑力的。

当我们采纳意向立场时，如果我们把任何传达真相的欲望归于 ChatGPT，我们将做出错误的预测。同样，将“幻觉”归于 ChatGPT 将导致我们预测它好像感知到了不存在的东西，而它所做的更像是为了听起来差不多而编造东西。前一种意向归因将导致我们试图纠正它的信念，并修复它的输入——这种策略即使有成功，也极为有限。另一方面，如果我们将硬核胡说八道者的意图归于 ChatGPT，我们将能更好地诊断它会犯错和传达谬误的情况。如果 ChatGPT 试图做任何事情，那就是试图将自己描绘成一个人。

由于这种认为 ChatGPT 是硬核胡说八道者的理由涉及对一个或多个关于心智和意义的有争议的观点做出承诺，它比仅仅将其视为一台胡说八道机器更具倾向性；但无论该程序是否具有意图，在线上的某个环节显然*存在*试图欺骗听者或读者关于事情本质的尝试，在我们看来，这足以证明称其输出为硬核胡说八道是合理的。

所以，尽管有必要提出这个附带说明，在我们看来，这似乎并未显著影响我们应该如何思考和谈论 ChatGPT 与胡说八道：使用它来炮制论文或演讲的人既不关心传达真相也不关心掩盖真相（因为这两者都需要关注真相*实际上*是什么），系统本身也是如此。最低限度，它大量产出软核胡说八道，并且，在某些关于意向归因性质的有争议的假设下，它产出硬核胡说八道；对我们而言，胡说八道的具体质地并不重要：无论哪种方式，ChatGPT 都是一个胡说八道者。

#### 胡说八道？幻觉？虚构？需要新术语

我们已经论证，我们应该使用胡说八道的术语，而不是“幻觉”来描述 ChatGPT 产生的言论。Edwards（2023）也注意到了“幻觉”术语不合适的建议，他更倾向于使用“虚构”一词。为什么我们的提议比这个或其他替代方案更好？

我们反对使用“幻觉”一词，因为它带有一些误导性的含义。当某人产生幻觉时，他们有一个非标准的感知体验，但实际上并没有感知到世界的某个特征（Macpherson, 2013），这里的“感知”被理解为一个成功术语，即他们实际上没有感知到那个物体或属性。这个术语对于大语言模型来说是不合适的，原因有多种。首先，正如 Edwards（2023）指出的，“幻觉”一词将大语言模型拟人化了。Edwards 还指出，将由此产生的问题归因于模型的“幻觉”，可能让创造者“将错误的输出归咎于人工智能模型，而不是为输出本身负责”，我们可能对这种推卸责任的行为保持警惕。大语言模型不会感知，所以它们肯定不会“错误感知”。其次，在大语言模型提供错误言论的情况下发生的事情，并不是它通常经历的过程的一种不寻常或偏离常态的形式（正如一些人声称在幻觉中是这样的，例如，关于感知的析取主义者）。当它的输出恰好是真实的时候，发生的是完全相同的过程。

关于“幻觉”就说这么多。那么 Edwards 偏爱的术语“虚构”呢？Edwards (2023) 说：

> 在人类心理学中，“虚构”发生在一个人的记忆出现空白，而大脑令人信服地填补了其余部分，却无意欺骗他人。ChatGPT 的工作方式不像人脑，但“虚构”一词可以说是一个更好的比喻，因为其中有一种创造性的填补空白原则在起作用 [……]。

正如 Edwards 指出的，这并不完美。再次，使用人类心理学术语有将大语言模型拟人化的风险。

这个术语也暗示了当大语言模型做出错误的言论时，发生了某种特殊情况，即，在这些场合——且仅在这些场合——它用错误的东西“填补”了记忆的空白。这也是误导性的。即使当 ChatGPT 确实给出我们正确的答案时，它的过程也是预测下一个标记。在我们看来，这错误地表明 ChatGPT 通常试图在其言论中传达准确的信息。但有充分的理由认为，它并没有它意图普遍分享的信念——例如，参见 Levinstein 和 Herrmann（即将发表）。在我们看来，这错误地表明 ChatGPT 通常试图在其言论中传达准确的信息。在它追踪真相的地方，它是间接地、偶然地这样做的。

这就是为什么我们倾向于将 ChatGPT 描述为一台胡说八道机器。这个术语避免了暗示在大语言模型的工作中发生了感知或记忆。我们也可以将其描述为每当它产生输出时都在胡说八道。就像人类的胡说八道者一样，一些输出可能是真实的，而另一些则不然。就像对待人类的胡说八道者一样，我们应该警惕依赖它的任何输出。

## 结论

投资者、政策制定者和普通公众如何对待这些机器以及如何对其做出反应，并非基于对其工作原理的深入技术理解，而是基于其能力和功能被传达的（通常是隐喻性的）方式。称它们的错误为“幻觉”并非无害：这容易导致一种混淆，即认为这些机器在某种程度上*错误感知*了，但仍然试图传达它们相信或感知到的东西。正如我们所论证的，这是错误的隐喻。这些机器并不试图传达它们相信或感知到的东西。它们的不准确性不是由于错误感知或幻觉。正如我们指出的，它们根本不试图传达信息。它们在胡说八道。

称聊天机器人的不准确性为“幻觉”，会助长技术吹捧者对其能力的过度炒作，并可能导致普通公众不必要的惊愕。它还暗示了解决不准确性问题的方法可能无效，并可能导致专家在人工智能对齐方面做出误导性的努力。当机器做对事情时，这也可能导致对它的错误态度：不准确性表明它在胡说八道，即使它做对的时候也是如此。称这些不准确性为“胡说八道”而非“幻觉”不仅更准确（正如我们所论证的）；在一个迫切需要它的领域，这是一种良好的科技传播。

## 修改历史

- ### 2024 年 7 月 11 日
    本文的一篇更正已发布：https://doi.org/10.1007/s10676-024-09785-3

## 注释

1.  费希特提出了一个特别令人惊讶的立场，他不仅将不作为的谎言视为撒谎，而且将明知他人处于错误认知下而*不予纠正*也视为撒谎。例如，如果我戴着假发，而有人相信这是我的真发，费希特认为这是谎言，我应为此负责。关于费希特立场的进一步讨论，请参见 Bacin (2021)。
2.  最初发表于 1986 年的《Raritan》杂志，VI(2)。此处对该作品的引用来自 2005 年的书籍版本。
3.  在做出这一评论时，法兰克福承认科恩所谓的“胡说八道”也配得上这个名字。在科恩的用法中（2002），胡说八道是一种无法阐明的文本，他将其与法国马克思主义者联系起来。近年来，其他几位作者也以各种方式探索了这一领域，每一位都为这场辩论增添了宝贵的见解。丹尼斯·惠特科姆和肯尼·伊斯瓦兰扩展了“胡说八道”可以应用的领域。惠特科姆认为可以有胡说八道的问题（以及命题），而伊斯瓦兰则认为我们可以富有成效地将某些活动视为胡说八道（2023）。

    虽然我们承认这些提供了宝贵的胡说八道见解，但我们将讨论限制在法兰克福的框架内。对于那些想更深入探讨这些区别的人，尼尔·利维的《哲学、胡说八道与同行评审》（2023）对现有的胡说八道进行了分类概述。

4.  这未必会破坏他们的目标。广告商可能意图给观众留下联想（例如，将“牛仔”或“勇敢”等积极想法与其香烟品牌联系起来），或加强/灌输品牌认知度。

    法兰克福将这种情况描述为发生在“闲聊会”中：“闲聊会的每个参与者……都依赖于一种普遍的认识，即他所表达或说的内容不应被理解为他全心全意地意指或明确相信为真的东西”（2005: 37）。然而，法兰克福声称闲聊会的内容与胡说八道是不同的。

5.  值得注意的是，我们所划分的硬核和软核胡说八道的区别，在科恩（2002）那里也有类似体现：他建议我们可以将胡说八道者视为“一个以胡说八道为目标的人，无论他命中目标的频率如何”，*或者*如果他们仅仅是“倾向于胡说八道：无论出于何种原因，都会产生大量无法阐明的东西”（p334）。虽然我们在此不采用科恩的说法，但他描述的特征与我们自己的描述之间的相似之处是惊人的。
6.  当然，石头也无法表达命题——但这里的担忧之一是 ChatGPT 是否真的*在*表达命题，或者它仅仅是行为主体表达命题的工具。更进一步的担忧是，我们甚至不应该将 ChatGPT 视为在表达命题——也许没有交流意图，因此我们应该将输出视为无意义的。即使接受这一点，我们仍然可以有意义地将它们视为表达命题。这一提议——关于聊天机器人的虚构主义——最近由马洛里（2023）讨论过。

## 参考文献

- Alkaissi, H., & McFarlane, S. I., (2023, February 19). Artificial hallucinations in ChatGPT: Implications in scientific writing. _Cureus, 15_(2), e35179. https://doi.org/10.7759/cureus.35179.
- Bacin, S. (2021). My duties and the morality of others: Lying, truth and the good example in Fichte’s normative perfectionism. In S. Bacin, & O. Ware (Eds.), _Fichte’s system of Ethics: A critical guide_. Cambridge University Press.
- Cassam, Q. (2019). _Vices of the mind_. Oxford University Press.
- Cohen, G. A. (2002). Deeper into bullshit. In S. Buss, & L. Overton (Eds.), _The contours of Agency: Essays on themes from Harry Frankfurt_. MIT Press.
- Davis, E., & Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. _Arxiv Preprint: arXiv, 2308_, 05713v2.
- Dennett, D. C. (1983). Intentional systems in cognitive ethology: The panglossian paradigm defended. _Behavioral and Brain Sciences_, _6_, 343–390.
- Dennett, D. C. (1987). _The intentional stance_. The MIT.
- Dennis Whitcomb (2023). Bullshit questions. _Analysis_, _83_(2), 299–304.
- Easwaran, K. (2023). Bullshit activities. _Analytic Philosophy_, _00_, 1–23. https://doi.org/10.1111/phib.12328.
- Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. _Ars Tecnica_. https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/, accesssed 19th April, 2024.
- Frankfurt, H. (2002). Reply to cohen. In S. Buss, & L. Overton (Eds.), _The contours of agency: Essays on themes from Harry Frankfurt_. MIT Press.
- Frankfurt, H. (2005). _On Bullshit_, Princeton.
- Knight, W. (2023). Some glimpse AGI in ChatGPT. others call it a mirage. _Wired_, August 18 2023, accessed via https://www.wired.com/story/chatgpt-agi-intelligence/.
- Levinstein, B. A., & Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. _Philosophical Studies_, 1–27.
- Levy, N. (2023). _Philosophy, Bullshit, and peer review_. Cambridge University.
- Lightman, H., et al. (2023). Let’s verify step by step. _Arxiv Preprint: arXiv_, _2305_, 20050.
- Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. _ArXiv Preprint: arXiv_, _2307_, 16850v1.
- Macpherson, F. (2013). The philosophy and psychology of hallucination: an introduction, in _Hallucination_, Macpherson and Platchias (Eds.), London: MIT Press.
- Mahon, J. E. (2015). The definition of lying and deception. _The Stanford Encyclopedia of Philosophy_ (Winter 2016 Edition), Edward N. Zalta (Ed.), https://plato.stanford.edu/archives/win2016/entries/lying-definition/.
- Mallory, F. (2023). Fictionalism about chatbots. _Ergo_, _10_(38), 1082–1100.
- Mandelkern, M., & Linzen, T. (2023). Do language models’ Words Refer?. _ArXiv Preprint: arXiv_, _2308_, 05576.
- OpenAI (2023). GPT-4 technical report. _ArXiv Preprint: arXiv_, _2303_, 08774v3.
- Proops, I., & Sorensen, R. (2023). Destigmatizing the exegetical attribution of lies: the case of kant. _Pacific Philosophical Quarterly_. https://doi.org/10.1111/papq.12442.
- Sarkar, A. (2023). ChatGPT 5 is on track to attain artificial general intelligence. _The Statesman_, April 12, 2023. Accesses via https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html.
- Shah, C., & Bender, E. M. (2022). Situating search. CHIIR ‘22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221–232 https://doi.org/10.1145/3498366.3505816.
- Weise, K., & Metz, C. (2023). When AI chatbots hallucinate. New York Times, May 9, 2023. Accessed via https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html.
- Weiser, B. (2023). Here’s what happens when your lawyer uses ChatGPT. _New York Times_, May 23, 2023. Accessed via https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html.
- Zhang (2023). How language model hallucinations can snowball. _ArXiv preprint: arXiv:_, _2305_, 13534v1.
- Zhu, T., et al. (2023). Large language models for information retrieval: A survey. _Arxiv Preprint: arXiv_, _2308_, 17107v2.

## 致谢

感谢 Neil McDonnell、Bryan Pickel、Fenner Tanswell 以及格拉斯哥大学大语言模型阅读小组的有益讨论和评论。

## 作者信息

### 作者及单位

1.  格拉斯哥大学，格拉斯哥，苏格兰

    Michael Townsen Hicks, James Humphries & Joe Slater

### 通讯作者

通信联系 Michael Townsen Hicks。

## 附加信息

### 出版商说明

施普林格·自然对出版地图和机构附属关系中的管辖权主张保持中立。

## 权利与许可

## **开放获取** 本文根据知识共享署名 4.0 国际许可协议进行许可，该协议允许以任何媒介或格式使用、共享、改编、分发和复制，只要您对原始作者和来源给予适当的署名，提供知识共享许可协议的链接，并说明是否进行了更改。本文中的图像或其他第三方材料包含在文章的知识共享许可协议中，除非材料的信用额度中另有说明。如果材料未包含在文章的知识共享许可协议中，并且您的预期用途不被法定法规允许或超出了允许的用途，您将需要直接从版权所有者那里获得许可。要查看此许可证的副本，请访问 http://creativecommons.org/licenses/by/4.0/

{{< embed-pdf-iframe file="s10676-024-09775-5.pdf" >}}
